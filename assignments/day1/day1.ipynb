{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you haven't installed mlxtend yet for ipython\n",
    "## UNCOMENT IF YOU HAVE ANACONDA or DOCKER\n",
    "!conda install -c rasbt mlxtend -qy\n",
    "### UNCOMENT IF YOU PIP INSTALLED EVERYTHING\n",
    "# !pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.evaluate import plot_decision_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron \n",
    "Implement the training portion of the SVM algorithm. Look to [slide 7 from the presentation](https://docs.google.com/a/berkeley.edu/presentation/d/1jaVNN3u7iyN3RGxAw9_61JH4qUio6moCxk1Id1jzYHY/edit?usp=sharinghttps://docs.google.com/a/berkeley.edu/presentation/d/1jaVNN3u7iyN3RGxAw9_61JH4qUio6moCxk1Id1jzYHY/edit?usp=sharing) on the mathematics of training a perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, eta=0.01, epochs=50):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def train(self, X, y):\n",
    "\n",
    "        self.w_ = np.random.randn(X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            ### FILL THIS IN ###\n",
    "            Z = np.zeros(X.shape[0])\n",
    "            for i in range(X.shape[0]):\n",
    "                Z[i] = self.predict(X[i])\n",
    "            V_index = np.where(Z != y)\n",
    "            \n",
    "            self.w_ = self.w_ + self.eta * np.dot(y[V_index], X[V_index])\n",
    "            error = np.size(V_index)\n",
    "            self.errors_.append(error)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "Dataset of three different Iris flower species and their respective features. [Source link](https://archive.ics.uci.edu/ml/datasets/Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# setosa and versicolor\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "\n",
    "# sepal length and petal length\n",
    "X = df.iloc[0:100, [0,2]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppn = Perceptron(epochs=10, eta=.01)\n",
    "\n",
    "ppn.train(X, y)\n",
    "print('Weights: %s' % ppn.w_)\n",
    "plot_decision_regions(X, y, clf=ppn)\n",
    "plt.title('Perceptron')\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_)+1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Missclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "This example is intended more as a demonstration of the sklearn api rather than a challenge to implement. SVMs are non-trivial to implement so we've omitted that here. If you're interested, [this article](http://tullo.ch/articles/svm-py/) covers an implementation using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.evaluate import plot_decision_regions\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# setosa and versicolor\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "\n",
    "# sepal length and petal length\n",
    "X = df.iloc[0:100, [0,2]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Margin SVM\n",
    "\n",
    "This calls the SVC class. This fits the standard sklearn api where each model has a `fit()` method which trains the model, and likewise has a `predict()` method that allows you to label new data. This particular implementation is _technically_ a soft-margin SVM, however, as the `C` parameter approaches infinity (and $10^{100}$ computationally is that case), the SVM becomes hard margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hard_svm = SVC(C=1e100,kernel='linear')\n",
    "hard_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_svm(clf, X, Y, title='SVM'):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(4, 8)\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "    # plot the parallels to the separating hyperplane that pass through the\n",
    "    # support vectors\n",
    "    b = clf.support_vectors_[0]\n",
    "    yy_down = a * xx + (b[1] - a * b[0])\n",
    "    b = clf.support_vectors_[-1]\n",
    "    yy_up = a * xx + (b[1] - a * b[0])\n",
    "\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    plt.plot(xx, yy, 'k-')\n",
    "    plt.plot(xx, yy_down, 'k--')\n",
    "    plt.plot(xx, yy_up, 'k--')\n",
    "\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "                s=80, facecolors='none')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n",
    "\n",
    "    print('Support Vectors: \\n%s' % clf.support_vectors_)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('sepal length [cm]')\n",
    "    plt.ylabel('petal length [cm]')\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_svm(hard_svm, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Margin SVM\n",
    "Now here's an example of a soft-margin SVM. The `C` parameter allows for slack variables. `C=1` is in fact the default argument, so you can omit it if you'd like. However, we've included it for your sake.\n",
    "\n",
    "As an exercise, you should experiment with different values of `C` and observe how that changes the result. \n",
    "\n",
    "Think of  a method for trying out a wide array of different values of `C`. This is part of an important tool in machine learning called [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization), where `C` is the hyperparamater of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
    "\n",
    "# setosa and versicolor\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-versicolor', -1, 1)\n",
    "\n",
    "# sepal length and petal length\n",
    "X = df.iloc[0:100, [0,2]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soft_svm = SVC(C=1,kernel='linear')\n",
    "soft_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_svm(soft_svm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
